{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/logs/*"
      ],
      "metadata": {
        "id": "-01O8l0PpFda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "Vk3l16BYpCxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/scalars"
      ],
      "metadata": {
        "id": "PyBNEGchpEJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# download and load dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_validation, y_validation) = mnist.load_data()\n",
        "\n",
        "# transform images to vectors (flatenning) and change type\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_validation = x_validation.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_validation = x_validation.astype('float32')\n",
        "\n",
        "# normalize vectors values from [0,255] -> [0,1]\n",
        "x_train /= 255\n",
        "x_validation /= 255\n",
        "\n",
        "print('train samples', x_train.shape)\n",
        "print('test samples', x_validation.shape)\n",
        "\n",
        "print('train label samples', y_train.shape)\n",
        "print('test label samples', y_validation.shape)"
      ],
      "metadata": {
        "id": "2ygJAp-8o0ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# convert labels from category to one-hot\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_validation = to_categorical(y_validation, num_classes=10)"
      ],
      "metadata": {
        "id": "_8yd6Jgzo320"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCH = 100\n",
        "\n",
        "# define each model training configuration\n",
        "experiments = [\n",
        "\n",
        "    # base\n",
        "    {'epoch': N_EPOCH, 'optimizer': tf.keras.optimizers.SGD, 'lr': 0.1, 'layers': [128, 64, 32], 'activation': 'relu',\n",
        "     'batch_size': 16, 'dropout': 0.5, 'BN': True},\n",
        "\n",
        "    # no dropout\n",
        "    {'epoch': N_EPOCH, 'optimizer': tf.keras.optimizers.SGD, 'lr': 0.1, 'layers': [128, 64, 32], 'activation': 'relu',\n",
        "     'batch_size': 16, 'dropout': 0.0, 'BN': True},\n",
        "\n",
        "    # deeper network\n",
        "    {'epoch': N_EPOCH, 'optimizer': tf.keras.optimizers.SGD, 'lr': 0.1, 'layers': [128, 64, 32, 16], 'activation': 'relu',\n",
        "     'batch_size': 16, 'dropout': 0.5, 'BN': True},\n",
        "\n",
        "    # batch size of 32\n",
        "    {'epoch': N_EPOCH, 'optimizer': tf.keras.optimizers.SGD, 'lr': 0.1, 'layers': [128, 64, 32], 'activation': 'relu',\n",
        "     'batch_size': 32, 'dropout': 0.5, 'BN': True},\n",
        "\n",
        "    # batch size of 64\n",
        "    {'epoch': N_EPOCH, 'optimizer': tf.keras.optimizers.SGD, 'lr': 0.1, 'layers': [128, 64, 32], 'activation': 'relu',\n",
        "     'batch_size': 64, 'dropout': 0.5, 'BN': True},\n",
        "\n",
        "    # activation function sigmoid\n",
        "    {'epoch': N_EPOCH, 'optimizer': tf.keras.optimizers.SGD, 'lr': 0.1, 'layers': [128, 64, 32], 'activation': 'sigmoid',\n",
        "     'batch_size': 16, 'dropout': 0.5, 'BN': True},\n",
        "\n",
        "    # no batch normalization\n",
        "    {'epoch': N_EPOCH, 'optimizer': tf.keras.optimizers.SGD, 'lr': 0.1, 'layers': [128, 64, 32], 'activation': 'relu',\n",
        "     'batch_size': 16, 'dropout': 0.5, 'BN': False},\n",
        "\n",
        "    # smaller learning rate\n",
        "    {'epoch': N_EPOCH, 'optimizer': tf.keras.optimizers.SGD, 'lr': 0.01, 'layers': [128, 64, 32], 'activation': 'relu',\n",
        "     'batch_size': 16, 'dropout': 0.5, 'BN': True},\n",
        "\n",
        "    # optimizer Adam\n",
        "    {'epoch': N_EPOCH, 'optimizer': tf.keras.optimizers.Adam, 'lr': 0.1, 'layers': [128, 64, 32], 'activation': 'relu',\n",
        "     'batch_size': 16, 'dropout': 0.5, 'BN': True},\n",
        "]"
      ],
      "metadata": {
        "id": "7GL3MVhUpIE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODELS = {}\n",
        "for i, config in enumerate(experiments):\n",
        "\n",
        "    # specify tensorboard directory\n",
        "    logdir = \"logs/scalars/\" + str(i)\n",
        "\n",
        "    # create tensorboard callback\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "    # create saving model callback\n",
        "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='model_{}.ckpt'.format(str(i)), monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "    # define model\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Dense(config['layers'][0], activation=config['activation'], input_dim=784))\n",
        "    for hiddden_dim in config['layers'][1:]:\n",
        "        if config['BN']:\n",
        "            model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.Dropout(config['dropout']))\n",
        "        model.add(tf.keras.layers.Dense(hiddden_dim, activation=config['activation']))\n",
        "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # define optimizer\n",
        "    optim = config['optimizer'](lr=config['lr'])\n",
        "\n",
        "    # compile model with optimizer, loss and metrics\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(x_train, y_train, validation_split=0.2, batch_size=config['batch_size'], epochs=config['epoch'], verbose=1, callbacks=[tensorboard_callback, checkpointer])\n",
        "\n",
        "    # add model to dictionary of models\n",
        "    MODELS.update({'experiment_{}'.format(str(i)): (history, model)})"
      ],
      "metadata": {
        "id": "xLSBY5KsubMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select model with best validation loss\n",
        "# the name should be set according to the validation accuracy\n",
        "history, model = MODELS['experiment_0']   # change name of model based on best validation accuracy"
      ],
      "metadata": {
        "id": "LOnjLx1p6Yxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# display best model prediction on validation examples\n",
        "for _ in range(10):\n",
        "\n",
        "    # display example\n",
        "    idx = random.choice(range(x_validation.shape[0]))\n",
        "    image = x_validation[idx].reshape(28, 28)\n",
        "    image = (image * 255).astype(dtype=np.uint8)\n",
        "    display(Image.fromarray(image).resize((128,128)))\n",
        "\n",
        "    # display model prediction\n",
        "    datapoint = x_validation[idx].reshape(1, 784)\n",
        "    predict_prob = model.predict(datapoint, verbose=0)\n",
        "    print('i am confident around {:.2f}% that this image corresponds to digit {}'.format(np.amax(predict_prob)*100, np.argmax(predict_prob)))"
      ],
      "metadata": {
        "id": "ovyZKeYi6Y0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_validation = x_validation.reshape(10000, 784)\n",
        "loss, acc = model.evaluate(x_validation, y_validation, batch_size=64, verbose=1)\n",
        "print('test accuracy: ', round(100*acc, 2))"
      ],
      "metadata": {
        "id": "BnUh0DwbD_gt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}