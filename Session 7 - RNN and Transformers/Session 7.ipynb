{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Links\n",
        "\n",
        "### APIs\n",
        "\n",
        "Keras-NLP : https://keras.io/api/keras_nlp/\n",
        "\n",
        "Tensorflow-NLP : https://www.tensorflow.org/api_docs/python/tfm/nlp\n",
        "\n",
        "Keras-NLP Transformer Encoder : https://keras.io/api/keras_nlp/modeling_layers/transformer_encoder/\n",
        "\n",
        "Keras-NLP position encoding : https://keras.io/api/keras_nlp/modeling_layers/sine_position_encoding/\n",
        "\n",
        "Tensorflow-NLP Transformer Encoder : https://www.tensorflow.org/api_docs/python/tfm/nlp/layers/TransformerEncoderBlock\n",
        "\n",
        "### Tutorials\n",
        "\n",
        "Keras NLP tutorials : https://keras.io/examples/nlp/\n",
        "\n",
        "Example of text classification with Transformers : https://keras.io/examples/nlp/text_classification_with_transformer/\n",
        "\n",
        "\n",
        "### Dataset\n",
        "\n",
        "Keras datasets : https://keras.io/api/datasets/\n",
        "\n",
        "Reuters newswire : https://keras.io/api/datasets/reuters/\n",
        "\n",
        "IMDB movie review sentiment : https://keras.io/api/datasets/imdb/"
      ],
      "metadata": {
        "id": "n4-n2on0GkWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/logs/*"
      ],
      "metadata": {
        "id": "BGljesdy_JiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "mRqlhAfE_KcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/scalars"
      ],
      "metadata": {
        "id": "KKGJNcZY_Lo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "zpA1jQi5p8xL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Download the dataset vocabulary, which can be used to limit the vocabulary size when downloading the dataset"
      ],
      "metadata": {
        "id": "P8jwLc6WnNox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "voc = keras.datasets.reuters.get_word_index()\n",
        "print(voc)\n",
        "print(len(voc))"
      ],
      "metadata": {
        "id": "rQhYq_1AnJUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reuters dataset classes from https://martin-thoma.com/nlp-reuters/\n",
        "\n",
        "'cocoa',\n",
        "'grain',\n",
        "'veg-oil',\n",
        "'earn',\n",
        "'acq',\n",
        "'wheat',\n",
        "'copper',\n",
        "'housing',\n",
        "'money-supply',\n",
        "'coffee',\n",
        "'sugar',\n",
        "'trade',\n",
        "'reserves',\n",
        "'ship',\n",
        "'cotton',\n",
        "'carcass',\n",
        "'crude',\n",
        "'nat-gas',\n",
        "'cpi',\n",
        "'money-fx',\n",
        "'interest',\n",
        "'gnp',\n",
        "'meal-feed',\n",
        "'alum',\n",
        "'oilseed',\n",
        "'gold',\n",
        "'tin',\n",
        "'strategic-metal',\n",
        "'livestock',\n",
        "'retail',\n",
        "'ipi',\n",
        "'iron-steel',\n",
        "'rubber',\n",
        "'heat',\n",
        "'jobs',\n",
        " 'lei',\n",
        " 'bop',\n",
        " 'zinc',\n",
        " 'orange',\n",
        " 'pet-chem',\n",
        " 'dlr',\n",
        " 'gas',\n",
        " 'silver',\n",
        " 'wpi',\n",
        " 'hog',\n",
        " 'lead'"
      ],
      "metadata": {
        "id": "fIh1_7OKpOhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the Reuters dataset while limiting the vocabulary size"
      ],
      "metadata": {
        "id": "XBka3PSXFCxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "max_features = 20000  # Only consider the top 20k words\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.reuters.load_data(num_words=max_features)"
      ],
      "metadata": {
        "id": "YcHHrh_3_STI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")"
      ],
      "metadata": {
        "id": "xHaF6boI4dtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To define the number of classes that we have in our dataset, we look at the maximum class index in the labels."
      ],
      "metadata": {
        "id": "xhfWPAMSjDd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This assume that all classes have at least one sample\n",
        "n_classes = max(y_train) + 1\n",
        "print(n_classes)"
      ],
      "metadata": {
        "id": "zzyYmg6eGZyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we want to train our model in batches, we need to pad the sequences so that they all have the same length. For this we have 2 possible solutions:\n",
        "\n",
        "*   we look at the longer sequence in our training set, and pad all the other sequences to have the same length everywhere\n",
        "*   or we can fix a maximum length and pad the sequences shorter than this length and trim the sequences longer than this length\n",
        "\n",
        "To make our model training faster, we select the second option; but feel free to try the first one and compare the performance.\n",
        "\n",
        "Hopefully, `tf.keras.preprocessing` provides us with a tool for padding sequences."
      ],
      "metadata": {
        "id": "E3_Z36LPFK9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 200  # Only consider the first 200 words of each newswire\n",
        "\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "ctIk3A8f85wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we convert our class indexes to one-hot vectors."
      ],
      "metadata": {
        "id": "0TVwcPLqjmh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = tf.one_hot(y_train, n_classes)\n",
        "y_val = tf.one_hot(y_val, n_classes)"
      ],
      "metadata": {
        "id": "66ZwQpLPjmoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras-NLP\n",
        "\n",
        "We will use the Keras-NLP API, because it provides us with the Positional Encoding ([SinePositionEncoding](https://keras.io/api/keras_nlp/modeling_layers/sine_position_encoding/)). We also use the [Transformer Encoder of Keras](https://keras.io/api/keras_nlp/layers/transformer_encoder/) that works similarly to Tensorflow one's.\n",
        "\n",
        "However here, the biggest difference is the use of the [Keras Embedding](https://keras.io/api/layers/core_layers/embedding/) layer that provides us with a masking option. This is extremly important because otherwise the padded values (that are just zeros) will be used in the self-attention of the encoder; while they do not mean anything but are just here to complete the sequences for batching. To make them invisible for the encoder we use **masking**. It consists to passing to the encoder a mask that contains boolean values with *True* where the encoder should use the vectors and *False* where it should not. The Embedding layer of Keras does this for us. We just have to pass to him `mask_zero=True`, and it will mask the zero values in the input. Note that you have to use Keras NLP layers after to be compatible with this option."
      ],
      "metadata": {
        "id": "TSRLkyLlFeqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets' install the Keras-NLP library"
      ],
      "metadata": {
        "id": "7OrxEA1UoVyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_nlp"
      ],
      "metadata": {
        "id": "Ik3jdfvMF9SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build your model."
      ],
      "metadata": {
        "id": "3hXbiIODoY3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_nlp\n",
        "from tensorflow import keras\n",
        "\n",
        "d_model = 64   # dimension of vectors in the Multi-Head Attention\n",
        "n_head = 4      # number of head in Multi-Head Attention\n",
        "d_ffn = 512     # dimension of vectors in the Feed Forward Network\n",
        "n_layer = 5     # number of encoder layers\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "\n",
        "x = keras.layers.Embedding(max_features, d_model, mask_zero=True)(inputs)   # Notice the mask_zero parameter to indicate to not pay attention to padding\n",
        "\n",
        "positional_encoding = keras_nlp.layers.SinePositionEncoding()(x)   # encode the position using Keras API\n",
        "\n",
        "x = x + positional_encoding   # add to the tokens\n",
        "\n",
        "for i in range(n_layer):\n",
        "    x = keras_nlp.layers.TransformerEncoder(intermediate_dim=d_ffn, num_heads=n_head, activation='relu')(x)\n",
        "\n",
        "x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "outputs = keras.layers.Dense(n_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "K-tDw6g2FtBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and train."
      ],
      "metadata": {
        "id": "xRffaV2Hoht0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "tensorboard_callback = TensorBoard(log_dir=\"logs/scalars/{}\".format(now))\n",
        "checkpointer = ModelCheckpoint(filepath='{}.keras'.format(now), monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=100, validation_data=(x_val, y_val), callbacks=[tensorboard_callback, checkpointer])"
      ],
      "metadata": {
        "id": "00pd9d5UGDud"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}