{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/logs/*"
      ],
      "metadata": {
        "id": "aMNwyuHvxYB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "laDvskPBxZi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/scalars"
      ],
      "metadata": {
        "id": "1Vql6kCaxbyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "(x_train, y_train), (x_validation, y_validation) = datasets.cifar100.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_validation.shape)\n",
        "print(y_train.shape)\n",
        "print(y_validation.shape)"
      ],
      "metadata": {
        "id": "p4ofYAAJIREa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "\n",
        "# preprocess function will normalize (mean + standard deviation) but not scale the images\n",
        "x_train = preprocess_input(x_train)\n",
        "x_validation = preprocess_input(x_validation)"
      ],
      "metadata": {
        "id": "U4hualuJ0FYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=100)\n",
        "y_validation = to_categorical(y_validation, num_classes=100)"
      ],
      "metadata": {
        "id": "q4TRvpMoRqn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import requests\n",
        "import tarfile\n",
        "\n",
        "# download and extract files from archive\n",
        "url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
        "response = requests.get(url, stream=True)\n",
        "file = tarfile.open(fileobj=response.raw, mode=\"r|gz\")\n",
        "file.extractall(path=\".\")\n",
        "\n",
        "with open('./cifar-100-python/meta', 'rb') as f:\n",
        "    labels = pickle.load(f)['fine_label_names']\n",
        "\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "jnTFWLXKJTFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "\n",
        "vgg19 = VGG19(include_top=False)\n",
        "print('VGG layers:')\n",
        "print(vgg19.summary())"
      ],
      "metadata": {
        "id": "nRgM60Ha-61H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "\n",
        "def load_VGG(transfer_strategy=None):\n",
        "\n",
        "    if transfer_strategy is None:\n",
        "        base_model = VGG19(weights=None, include_top=False, input_shape=(32,32,3))\n",
        "    else:\n",
        "        base_model = VGG19(weights='imagenet', include_top=False, input_shape=(32,32,3))\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(base_model)\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(100, activation='softmax'))\n",
        "\n",
        "    if not (transfer_strategy is None) and transfer_strategy != 'imagenet':\n",
        "        trainable = False\n",
        "\n",
        "        # iterate over layers taken from VGG19 (= model.layers[0])\n",
        "        for layer in model.layers[0].layers:\n",
        "            if transfer_strategy in layer.name and not trainable:\n",
        "                print('start unfreezing from layer {}'.format(layer.name))\n",
        "                trainable = True\n",
        "\n",
        "            # if freeze is False, then set layer to trainable\n",
        "            layer.trainable = trainable\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "srCK6N9_8pBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "\n",
        "# 3 transfer learning strategy are applied:\n",
        "#   - no transfer at all (train from scratch)\n",
        "#   - use ImageNet pre-trained weights and re-train all the model\n",
        "#   - use ImageNet pre-trained weights and re-train only layers from block 4\n",
        "\n",
        "for transfer_strategy in [None, 'imagenet', 'block5']:\n",
        "\n",
        "    tensorboard_callback = TensorBoard(log_dir=\"logs/scalars/{}\".format(transfer_strategy))\n",
        "\n",
        "    checkpointer = ModelCheckpoint(filepath='model_{}.keras'.format(transfer_strategy), monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "    model = load_VGG(transfer_strategy)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, validation_data=(x_validation, y_validation), batch_size=16, epochs=10, callbacks=[tensorboard_callback, checkpointer])"
      ],
      "metadata": {
        "id": "hT7rYMppHRPj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}